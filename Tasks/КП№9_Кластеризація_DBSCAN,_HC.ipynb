{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNhBviOWN-YW"
      },
      "source": [
        "<center><font size=\"6\"><b>Комп'ютерний практикум 9.\n",
        "\n",
        "<center><b> Методи кластеризації </font>\n",
        "\n",
        "\n",
        "<center><b><i><font size=\"4\"> DBSCAN (Density-Based Clustering)\n",
        "\n",
        "Hierarchical Clustering\n",
        "\n",
        "\n",
        "\n",
        "</b></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtM5UAzB8LpF"
      },
      "source": [
        "##__DBSCAN (Density-Based Clustering)__\n",
        "\n",
        ">__DBSCAN (Density-based spatial clustering of applications with noise)__\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/max/1013/1*tc8UF-h0nQqUfLC8-0uInQ.gif\" width=\"600\"></center>\n",
        "\n",
        "Даний метод базується на використанні щільності даних. На вхід він потребує матрицю близькості та два параметри: $\\epsilon$-окіл (максимальна відстань між об'єктами) та $m$ кількість сусідів (мінімальна кількість елементів, що знаходиться в $\\epsilon$-околі даного елемента для створення кластеру). \n",
        "1. Область $E(x)$, для якої $\\forall y: \\rho(x, y) \\leq \\epsilon$ назвемо $\\epsilon$-околом об'єкта $x$.\n",
        "2. Кореневим об'єктом або ядерним об'єктом степеня $m$ називається об'єкт, $\\epsilon$-окіл якого містить не менше $m$ об'єктів: $|E(x)| \\geq m$.\n",
        "3. Об'єкт $p$ безпосередньо щільно-доясяжний із об'єкта $q$, якщо $p \\in E(q)$ та $q$ — кореневий об'єкт.\n",
        "4. Объект $p$ щільно-доясяжний із об'єкта $q$, якщо $\\exists p_1, p_2 \\dots p_n, p_1 = q, p_n = p$, такі що $\\forall i \\in 1 \\dots n-1: p_{i+1}$ безпосередньо щільно-досяжний з $p_i$\n",
        "5. шум: всі елементи, недосяжні з будь-якого іншого елементу вважаються шумом\n",
        "\n",
        "###__Етапи алгоритму:__\n",
        "1. Алгоритм починається з довільної точки, яка була відвідана, і інформація про її окіл береться з параметру.\n",
        "2. Якщо ця точка містить $m$ сусідів в $\\epsilon$-околі, починається формування кластера. В іншому випадку точка позначається як шум. Ця точка може бути пізніше знайдена в $\\epsilon$-околі іншої точки і, таким чином, може стати частиною іншого кластера. Тут важлива концепція досяжності щільності і точок, пов'язаних щільністю.\n",
        "3. Якщо точка знайдена як коренева точка, то точки в $\\epsilon$-околі також є частиною кластера. Таким чином, всі точки, знайдені в $\\epsilon$-околі, додаються разом з їх власним $\\epsilon$-околом, якщо вони також є кореневими точками.\n",
        "4. Процес триває до того часу, поки кластер, пов'язаний щільністю, не сформується повністю.\n",
        "5. Процес відновлюється з нової точки, яка може бути частиною утвореного кластеру або позначена як шум.\n",
        "\n",
        "\n",
        "<center><img src=\"https://www.machinelearningmastery.ru/img/0-166263-693665.png\" width=\"400\"></center>\n",
        "\n",
        "<center><img src=\"https://hsto.org/r/w1560/files/0ba/54b/abe/0ba54babe162458bb9f8e004e5329cfa.png\" width=\"400\"></center>\n",
        "\n",
        "\n",
        "### __Переваги методу__\n",
        "\n",
        "1. Знаходить кластери різної форми\n",
        "2. Ефективний для великої бази даних\n",
        "3. Визначає кількість кластерів самостійно\n",
        "4. Знаходить викиди (шуми)\n",
        "\n",
        "\n",
        "### __Недоліки методу__\n",
        "\n",
        "1. Якщо в базі даних є дані, які утворюють кластери різної щільності, то DBSCAN не вдається добре кластеризувати такі дані, оскільки кластеризація залежить від параметрів $\\epsilon$-околу та $m$ сусідів, вони можуть бути обрані окремо для усих кластерів.\n",
        "2. Якщо дані та функції не зовсім зрозумілі експерту в області, то налаштування параметрів $\\epsilon$-околу та $m$ сусідів може бути достатньо складним і, можливо, знадобиться порівняння для кількох ітерацій з різними значеннями $\\epsilon$ та $m$.\n",
        "3. $O(N\\log{N})$-складність\n",
        "4. Пограничні точки різних кластерів моуть бути хибно класифіковані\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUBEZYM2Vi7D"
      },
      "source": [
        "##__Реалізація алгоритму DBSCAN__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSxaxbOfQT9K"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSZnBBBNGHnO"
      },
      "source": [
        "__Завантажимо дані__\n",
        "\n",
        "Скористаємося даними з пакету `sklearn` `make_moons`.\n",
        "\n",
        "Візьмемо 200 точок та додамо деяку зашумленість (гаусів шум)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fwRvv_3Np1f"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "if __name__ == \"__main__\":\n",
        "       \n",
        "    x,y = make_moons(n_samples=200,noise=0.1,random_state=0)\n",
        "    plt.scatter(x[:,0],x[:,1],c=\"blue\",marker=\"o\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILTN9nEezMZF"
      },
      "source": [
        "> імпортуємо з пакету `sklearn.cluster` функцію `DBSCAN`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    from sklearn.cluster import DBSCAN"
      ],
      "metadata": {
        "id": "GTHU3yrbWAMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###__Моделювання__\n",
        "> Задамо параметри функції `DBSCAN`:\n",
        "\n",
        " `eps`: $\\epsilon$-окіл\n",
        "\n",
        " `min_samples`: кількість $m$ сусідів\n",
        "\n",
        " та метрику -  `metric`"
      ],
      "metadata": {
        "id": "gK_sLjUpWfnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.2,min_samples=5,metric=\"euclidean\")"
      ],
      "metadata": {
        "id": "krDuXh_2WcgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Проведемо навчання моделі та побудуємо результат"
      ],
      "metadata": {
        "id": "2jO26Q7lXZZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan_y = dbscan.fit_predict(x)"
      ],
      "metadata": {
        "id": "kyC8K5JJYdK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC8cLnyL6WT3"
      },
      "source": [
        "# Точки кластеру 1\n",
        "plt.scatter (x [dbscan_y == 0,0], x [dbscan_y == 0,1], c = \"green\", marker = \"o\", label = \"cluster 1\")\n",
        "# Точки кластеру 2\n",
        "plt.scatter (x [dbscan_y == 1,0], x [dbscan_y == 1,1], c = \"red\", marker = \"s\", label = \"cluster 2\")\n",
        "# Викиди (шум)\n",
        "plt.scatter (x [dbscan_y == - 1,0], x [dbscan_y == - 1,1], c = \"blue\", marker = \"^\", label = \"noise spot\")\n",
        "# Заголовок діаграми\n",
        "plt.title (\"Density-Based Clustering\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acI7FAeDa2YC"
      },
      "source": [
        "##__Hierarchical Clustering__\n",
        "\n",
        ">__Hierarchical Clustering__\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/max/1050/1*ET8kCcPpr893vNZFs8j4xg.gif\" width=\"600\"></center>\n",
        "\n",
        "Даний метод базується на використанні відстаней між елементами та принципами побудови дерева\n",
        "\n",
        "Розрізняють:\n",
        "\n",
        "* _Агломеративний підхід_. (Підхід \"знизу - вгору\") Починається алгоритм з того, що кожен об'єкт - окремий кластер. На кожному кроці відбувається об'єднання два найближчі кластери в один та зупиняємося тоді, коли всі об'єкти будуть знаходитися в одному кластері\n",
        "\n",
        "* _Дивізивний підхід_ (Оберенений до агломеративного, \"згори - вниз\"). На вхід подаються всі дані як один кластер. На кожному кроці відбувається розбиття на кластери доти поки всі елементи не стануть елементами персонального кластеру\n",
        "\n",
        "Більш популярний Агломеративний підхід, тому зосередимо увагу на ньому.\n",
        "\n",
        "Оскільки в даному методі розраховується близькість між елементами, то звісно, необхідно задавати метрику.\n",
        "\n",
        ">Відстань між кластером та всима іншими кластерами можна задавати різними способами (\"__Linkage__\"):\n",
        "* __Single Linkage__ - відстань між кластерами обчислюється як мінімальне між всима парами об'єктів з різних кластерів.\n",
        "* __Complete Linkage__ - відстань між кластерами обчислюється як максимальне між парами об'єктів з різних кластерів.\n",
        "* __Average Linkage__ - середня відстань між всима парами об'єктів з різних кластерів.\n",
        "* __Centroid Linkage__ - відстань між центроїдами різних кластерів\n",
        "* __Ward Linkage__ - модифікований попередній метод з врахуванням розмірів самих кластерів.\n",
        "\n",
        "Візуалізація ієрархіїї, де вісь абсцис - обє'кти, а вісь ординат - відстань, на якій відповідні об'єкти, кластери, об'єднані один з одним, називається __дендрограма__\n",
        "\n",
        "###__Етапи алгоритму:__\n",
        "1. Розраховується відстань між елементами.\n",
        "2. Для об'єднання в новий кластер обирається два найближчих елементи. Середнє значення двук точок обирається як нове значення та класифікується в одну категорію\n",
        "3. Перераховується схожіссть між новим кластером та кожним \"старим\".\n",
        "4. Повторюються кроки 2 та 3\n",
        "\n",
        "\n",
        "### __Переваги методу__\n",
        "\n",
        "1. Отримується не одне розбиття на кластери, а ціла ієрархія кластерів\n",
        "2. Візуалізація всих етапів кластеризації на дендрограмі\n",
        "3. Низька чуттєвість до шумів\n",
        "\n",
        "\n",
        "### __Недоліки методу__\n",
        "\n",
        "1. Вибір оптимальної кількості кластерів.\n",
        "3. Для великої кількості даних працює достатньо довго з більшими затрати=ами пам'яті. Складність алгоритму - $O(N^2\\log{N})$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soHUgVhH7LR2"
      },
      "source": [
        "> імпортуємо з пакету `sklearn.cluster` функцію `AgglomerativeClustering`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ],
      "metadata": {
        "id": "dsj8riTq7HBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###__Моделювання__\n",
        "> Задамо параметри функції `AgglomerativeClustering`:\n",
        "\n",
        " `n_clusters`: кількість кластерів, які необхідно знайти\n",
        "\n",
        " `affinity` - метрика\n",
        "\n",
        " `linkage` - критерій зв'язку"
      ],
      "metadata": {
        "id": "-t_LhLqz68JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ac = AgglomerativeClustering(n_clusters=2,affinity=\"euclidean\",linkage=\"complete\")"
      ],
      "metadata": {
        "id": "Sr7cERdj8XcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Проведемо навчання моделі та побудуємо результат"
      ],
      "metadata": {
        "id": "qCwsl-2n8TFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ac_y = ac.fit_predict(x)\n",
        "     "
      ],
      "metadata": {
        "id": "qWxFQ9Pu67la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Точки кластеру 1\n",
        "plt.scatter (x [ac_y == 0,0], x [ac_y == 0,1], c = \"green\", marker = \"o\", label = \"cluster 1\")\n",
        "# Точки кластеру 2\n",
        "plt.scatter (x [ac_y == 1,0], x [ac_y == 1,1], c = \"red\", marker = \"s\", label = \"cluster 2\")\n",
        "# Заголовок\n",
        "plt.title (\"Hierarchical Clustering\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2gqAYAYg8eJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Дендрограма__"
      ],
      "metadata": {
        "id": "wvP9P6oH97cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Побудуємо матрицю відстаней між елементами"
      ],
      "metadata": {
        "id": "Gc8B87pG-b3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance_matrix \n",
        "dist_matrix = distance_matrix(x,x) \n",
        "print(dist_matrix)"
      ],
      "metadata": {
        "id": "_rrbBQGh9OKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Зстосуємо метод зв'язку між кластерами"
      ],
      "metadata": {
        "id": "GM-5BgGX-kYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster import hierarchy \n",
        "Z = hierarchy.linkage(dist_matrix, 'complete')"
      ],
      "metadata": {
        "id": "GKUC7qDD9etg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Побудуємо дендрограму"
      ],
      "metadata": {
        "id": "gaEQPdG1_Axy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dendro = hierarchy.dendrogram(Z)"
      ],
      "metadata": {
        "id": "nx-dL23L9sx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEoeTCVzgj0O"
      },
      "source": [
        "##<center>__Самостійні завдання__</center>\n",
        "\n",
        "> Скопіювати блок самостійних завдань в окремий файл ***LastName_CP9.ipynb***\n",
        "\n",
        "> Інсталюйте необхідні пакети бібліотек Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BDe0NyTgmSY"
      },
      "source": [
        "### Завдання №1\n",
        "\n",
        "* Завантажте дані з лінку\n",
        "\n",
        "`url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"`\n",
        "\n",
        "<center><img src=\"http://sebastianraschka.com/images/blog/2015/principal_component_analysis_files/iris.png\" width=\"600\" alt=\"content-vs-colab.png\"></center>\n",
        "\n",
        "* задайте наступні імена колонок датафрейму:\n",
        "\n",
        "`names=['sepal length','sepal width','petal length','petal width','class label']`\n",
        "* виведіть описову статистику датасету\n",
        "* сформуйте масив характеристик $X$, використовуйте лише `sepal length` та `sepal width` \n",
        "* Побудуйте точковий графік даних характеристик відносно відомих класів\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0XIjMe_gpOr"
      },
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання №2\n",
        "\n",
        "* Проведіть стандартизацію даних\n",
        "* застосуйте алгоритм $k$-means для $k=2$, $k=3$, $k=4$ з оптимізацією та без, побудуйте діаграми кластерів\n",
        "* порівняйте результати кластеризації при $k=3$ з діаграмою класів\n",
        "* побудуйте залежність кількості кластерів $k$ та квадратичного критерію інерції \n",
        "* зробіть висновки"
      ],
      "metadata": {
        "id": "54bd3-Sm2-ze"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5xD3xxVpvNI"
      },
      "source": [
        "### Завдання №3\n",
        "\n",
        "* застосуйте алгоритм `DBSCAN`та побудуйте діаграму кластерів\n",
        "* зробіть висновки\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_V_77Igpzgi"
      },
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Завдання №4\n",
        "\n",
        "* застосуйте алгоритм агломеративної ієрархії та побудуйте діаграму кластерів\n",
        "* побудуйте дендрограму\n",
        "* зробіть висновки\n"
      ],
      "metadata": {
        "id": "cI8hnPxS_nTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ"
      ],
      "metadata": {
        "id": "1P3juKxvAGS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Завдання №5\n",
        "\n",
        "* порівняйте результати кластеризації методами $k$-means, DBSCAN та агломеративної ієрархії  \n",
        "* зробіть висновки"
      ],
      "metadata": {
        "id": "TmvSFCOHADBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# МІСЦЕ ДЛЯ КОДУ"
      ],
      "metadata": {
        "id": "w4t4lp2QAIW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}